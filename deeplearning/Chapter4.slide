ゼロから作るDeep Learning
Chapter 4 Learning approach
21 Apr 2017

@_Neetless_

* gradient descent & loss function

* what happens when the learning rate changes 1

- large learning rate: swing learning
- small learning rate: slow learning

.image ./img/lr0_1.png 200 _
.image ./img/lr0_01.png 200 _

* what happens when the learning rate chage 2

- large learning rate: difficalt to reach minimum
- small learning rate: get loacl minimum

.video ./img/large_learning_rate.mp4 video/mp4 200 _
.video ./img/small_learning_rate.mp4 video/mp4 200 _

* differntiation

- numerical diffrentiation
- analytical differentiation

*most* *important* *thing* *is* *speed*

Processor Name: Intel Core i7
Processor Speed: 2.5 GHz

- numerical differentiation with 1 iteration
CPU times: user 40 s, sys: 4.83 s, total: 44.8 s
Wall time: 44.2 s

- backpropagation with 10000 iteration
CPU times: user 32.1 s, sys: 6.31 s, total: 38.4 s
Wall time: 23.5 s

* learning algorithm

- *SGD*(stochastic gradient descent)

1. mini-batch
2. calculate gradient
3. update parameter
4. repeat 1-3

* batch & epoch

- batch is for SGD. i.e. update parameters with training loss..

- epoch is for confirming generalization performance. i.e. avoid over fitting with validation loss.
